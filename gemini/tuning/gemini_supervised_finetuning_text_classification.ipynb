{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Supervised Fine-tuning for Text Classification with Gemini\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/gemini_supervised_finetuning_text_classification.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Ftuning%2Fgemini_supervised_finetuning_text_classification.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/tuning/gemini_supervised_finetuning_text_classification.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/tuning/gemini_supervised_finetuning_text_classification.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy3a0zd7phg8"
   },
   "source": [
    "| | | |\n",
    "|-|-|-|\n",
    "|Author(s) | [Gabriela Hernandez Larios](https://github.com/gabrielahrlr) | [Elia Secchi](https://github.com/eliasecchig)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to perform text classification with Gemini models. From in-context learning (using zero-shot and few-shot learning) to in-weights learning fine-tuning Gemini models for text classification.\n",
    "\n",
    "### Objective\n",
    " We'll cover the development cycle from preparing the dataset, to setting up an evaluation framework to perform text classification tasks using Gemini. Additionally, you'll learn how to create and log experiments, adapting Gemini models to the text classification task with in-context and in-weights (fine-tuning) learning approaches, and compare the performances.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML Services and Resources:\n",
    "\n",
    "- Google Cloud Storage\n",
    "- Vertex AI Experiments\n",
    "- Vertex AI Fine-Tuning\n",
    "- Gemini 1.0 Pro\n",
    "\n",
    "The steps performed include:\n",
    "- [Load and split dataset](#scrollTo=EdvJRUWRNGHE&line=1&uniqifier=1)\n",
    "- [Evaluation and Experiment Setup](#scrollTo=c2YOsromfcuB&line=6&uniqifier=1)\n",
    "- [In-Context learning (zero-shot and few-shot) using Gemini Models](#scrollTo=EfKnRU-SfcuB)\n",
    "- [Fine-tuning Gemini 1.0 Pro for text classification](#scrollTo=Qs9eHiL5fcuD)\n",
    "- [Comparative Evaluation]()\n",
    "- [[Optional] Heuristics for computing Confidence Scores](#scrollTo=KW7wPWQWuQT4)\n",
    "\n",
    "### Dataset\n",
    "The [BBC News dataset](http://mlg.ucd.ie/datasets/bbc.html) consists of 2225 articles from the BBC news website corresponding to five topical areas: business, entertainment, politics, sport, and tech.  This dataset was downloaded from http://mlg.ucd.ie/datasets/bbc.html\n",
    "\n",
    "**Dataset Citation**\n",
    "\n",
    "```\n",
    "@inproceedings{greene06icml,\n",
    "\tAuthor = {Derek Greene and P\\'{a}draig Cunningham},\n",
    "\tBooktitle = {Proc. 23rd International Conference on Machine learning (ICML'06)},\n",
    "\tPages = {377--384},\n",
    "\tPublisher = {ACM Press},\n",
    "\tTitle = {Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering},\n",
    "\tYear = {2006}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet google-cloud-aiplatform datasets backoff multiprocess gcsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILS7NNudfct_"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XjsxAjgJfct_"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "# Data Handling and Processing\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "\n",
    "# Google Cloud Libraries\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    GenerativeModel,\n",
    "    GenerationConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")\n",
    "from vertexai.preview.tuning import sft\n",
    "\n",
    "# Multiprocessing\n",
    "import multiprocess as mp\n",
    "from tqdm import tqdm\n",
    "import backoff\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information, initialize Vertex AI SDK for Python and create a GCS bucket\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5r1Bhf7fcuA"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSZS8QfOfcuA"
   },
   "source": [
    "**warning:** Only if your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXxi2tC2fcuA"
   },
   "outputs": [],
   "source": [
    "!gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSZ8-hwafcuA"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAEAlYH2fcuA"
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6sph8etfcuA"
   },
   "source": [
    "#### Batch Prediction - Helper functions\n",
    "\n",
    "These helper functions streamline batch predictions using parallelization and multithreading with online Gemini Models. Gemini also offers the possibility to [perform batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) in **Public Preview** (July 2024). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Fz4tGANnfcuA"
   },
   "outputs": [],
   "source": [
    "def backoff_hdlr(details) -> None:\n",
    "    \"\"\"\n",
    "    Handles backoff events.\n",
    "\n",
    "    Args:\n",
    "        details: A dictionary containing information about the backoff event.\n",
    "    \"\"\"\n",
    "    print(f\"Backing off {details['wait']:.1f} seconds after {details['tries']} tries\")\n",
    "\n",
    "\n",
    "def log_error(msg: str, *args: Any) -> None:\n",
    "    \"\"\"\n",
    "    Logs an error message and raises an exception.\n",
    "\n",
    "    Args:\n",
    "        msg: The error message.\n",
    "        *args: Additional arguments to be passed to the logger.\n",
    "    \"\"\"\n",
    "    mp.get_logger().error(msg, *args)\n",
    "    raise Exception(msg)\n",
    "\n",
    "\n",
    "def handle_exception_threading(f: Callable) -> Callable:\n",
    "    \"\"\"\n",
    "    A decorator that handles exceptions in a threaded environment.\n",
    "\n",
    "    Args:\n",
    "        f: The function to decorate.\n",
    "\n",
    "    Returns:\n",
    "        The decorated function.\n",
    "    \"\"\"\n",
    "\n",
    "    def applicator(*args: Any, **kwargs: Any) -> Any:\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            log_error(traceback.format_exc())\n",
    "\n",
    "    return applicator\n",
    "\n",
    "\n",
    "@handle_exception_threading\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, ResourceExhausted, max_tries=30, on_backoff=backoff_hdlr\n",
    ")\n",
    "def _predict_message(message: str, model: GenerativeModel) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Predict messages\n",
    "\n",
    "    Args:\n",
    "        message: The message to predict.\n",
    "        model: The GenerativeModel to use for prediction.\n",
    "\n",
    "    Returns:\n",
    "        The predicted message, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    response = model.generate_content([message], stream=False)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def batch_predict(\n",
    "    messages: List[str], model: GenerativeModel, max_workers: int = 4\n",
    ") -> List[Optional[str]]:\n",
    "    \"\"\"\n",
    "    Predicts the classes for a list of messages\n",
    "\n",
    "    Args:\n",
    "        - messages: list of all messages to predict\n",
    "        - model: model to use for predicting.\n",
    "        - max_workers: number of workers to use for parallel predictions\n",
    "\n",
    "    Returns:\n",
    "        - list of predicted labels\n",
    "\n",
    "    \"\"\"\n",
    "    predictions = list()\n",
    "    with ThreadPoolExecutor(max_workers) as pool:\n",
    "        partial_func = partial(_predict_message, model=model)\n",
    "        for message in tqdm(pool.map(partial_func, messages), total=len(messages)):\n",
    "            predictions.append(message)\n",
    "            pass\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tKVjsJKfcuA"
   },
   "source": [
    "#### Vertex AI Experiment Helper\n",
    "We will define a `VertexAIExperimentManager` class to simplify the creation, logging and runs management of experiments using Vertex AI Experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexAIExperimentManager:\n",
    "    \"\"\"\n",
    "    A class for managing experiments and runs in Vertex AI.\n",
    "    This class encapsulates the functionality for creating experiments, logging runs,\n",
    "    and retrieving experiment data in Vertex AI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, project: str, location: str):\n",
    "        self.project = project\n",
    "        self.location = location\n",
    "        self.current_experiment = None\n",
    "\n",
    "    def init_experiment(\n",
    "        self, experiment_name: str, experiment_description: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Initialize or switch to a specific experiment.\"\"\"\n",
    "        self.current_experiment = experiment_name\n",
    "        aiplatform.init(\n",
    "            experiment=experiment_name,\n",
    "            experiment_description=experiment_description,\n",
    "            experiment_tensorboard=False,\n",
    "            project=self.project,\n",
    "            location=self.location,\n",
    "        )\n",
    "\n",
    "    def create_experiment(\n",
    "        self, experiment_name: str, experiment_description: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Create an Experiment on Vertex AI Experiments\"\"\"\n",
    "        self.init_experiment(experiment_name, experiment_description)\n",
    "\n",
    "    def log_run(\n",
    "        self, run_name: str, params: Dict[str, Any], metrics: Dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"\"\"Log experiment run data to Vertex AI Experiments.\"\"\"\n",
    "        if not self.current_experiment:\n",
    "            raise ValueError(\"No experiment initialized. Call init_experiment first.\")\n",
    "\n",
    "        aiplatform.start_run(run=run_name)\n",
    "        aiplatform.log_params(params)\n",
    "        aiplatform.log_metrics(metrics)\n",
    "        aiplatform.end_run()\n",
    "\n",
    "    def get_experiments_data_frame(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Retrieve a DataFrame of experiment data from Vertex AI Experiments.\"\"\"\n",
    "        if not self.current_experiment:\n",
    "            raise ValueError(\"No experiment initialized. Call init_experiment first.\")\n",
    "\n",
    "        return aiplatform.get_experiment_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Helper Functions for Data Transformation \n",
    "This section contains helper functions designated to transform data from different formats into the specific format required for fine-tuning Gemini models on Vertex AI. These functions handle:\n",
    "\n",
    "- Pandas DataFrames\n",
    "- CSV files previously used for training AutoML text classifiers\n",
    "- JSONL files previously used for training AutoML text classifiers\n",
    "\n",
    "It also includes a function to validate the transformed dataset, ensuring it adheres to the correct format and roles for Gemini fine-tuning on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gemini_messages(\n",
    "    text: str, label: str, system_prompt: Optional[str] = None\n",
    ") -> dict:\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.extend(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "            {\"role\": \"model\", \"content\": label},\n",
    "        ]\n",
    "    )\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "def prepare_tuning_dataset_from_df(\n",
    "    tuning_df: pd.DataFrame, system_prompt: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares a tuning dataset from a pandas DataFrame for Gemini fine-tuning.\n",
    "    Args:\n",
    "        tuning_df: A pandas DataFrame with columns \"text\" and \"label_text\".\n",
    "        system_prompt: An optional system prompt for zero-shot learning.\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the data in the Gemini tuning format.\n",
    "    \"\"\"\n",
    "    tuning_dataset = [\n",
    "        create_gemini_messages(row[\"text\"], row[\"label_text\"], system_prompt)\n",
    "        for _, row in tuning_df.iterrows()\n",
    "    ]\n",
    "    return pd.DataFrame(tuning_dataset)\n",
    "\n",
    "\n",
    "def convert_tuning_dataset_from_automl_csv(\n",
    "    automl_gcs_csv_path: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    partition: str = \"training\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts an AutoML CSV dataset for text classification to the Gemini tuning format.\n",
    "    Args:\n",
    "        automl_gcs_csv_path: The GCS path to the AutoML CSV dataset.\n",
    "        system_prompt: The instructions to the model.\n",
    "        partition: The partition to extract from the dataset (e.g., \"training\", \"validation\", \"test\"). Defaults to \"training\".\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the data in the Gemini tuning format.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(automl_gcs_csv_path, names=[\"partition\", \"text\", \"label\"])\n",
    "    df_automl = df.loc[df[\"partition\"] == partition]\n",
    "    gemini_dataset = [\n",
    "        create_gemini_messages(row[\"text\"], row[\"label\"], system_prompt)\n",
    "        for _, row in df_automl.iterrows()\n",
    "    ]\n",
    "    return pd.DataFrame(gemini_dataset)\n",
    "\n",
    "\n",
    "def convert_tuning_dataset_from_automl_jsonl(\n",
    "    project_id: str,\n",
    "    automl_gcs_jsonl_path: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    partition: str = \"training\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts an AutoML JSONL dataset for text classification to the Gemini tuning format.\n",
    "    Args:\n",
    "        automl_gcs_jsonl_path: The GCS path to the AutoML JSONL dataset for text classification.\n",
    "        system_prompt: The instructions to the model.\n",
    "        partition: The partition to extract from the dataset (e.g., \"training\", \"validation\", \"test\"). Defaults to \"training\".\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the data in the Gemini tuning format.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    gcs_file_system = gcsfs.GCSFileSystem(project=project_id)\n",
    "    with gcs_file_system.open(automl_gcs_jsonl_path) as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            processed_data.append(\n",
    "                {\n",
    "                    \"label\": data[\"classificationAnnotation\"][\"displayName\"],\n",
    "                    \"text\": data[\"textContent\"],\n",
    "                    \"partition\": data[\"dataItemResourceLabels\"][\n",
    "                        \"aiplatform.googleapis.com/ml_use\"\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    df_automl = df.loc[df[\"partition\"] == partition]\n",
    "    gemini_dataset = [\n",
    "        create_gemini_messages(row[\"text\"], row[\"label\"], system_prompt)\n",
    "        for _, row in df_automl.iterrows()\n",
    "    ]\n",
    "    return pd.DataFrame(gemini_dataset)\n",
    "\n",
    "\n",
    "def validate_gemini_tuning_jsonl(gcs_jsonl_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Validates a JSONL file on Google Cloud Storage against the Gemini tuning format.\n",
    "\n",
    "    Args:\n",
    "        gcs_jsonl_path: The GCS path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries representing the errors found in the file.\n",
    "        Each dictionary has the following structure:\n",
    "        {\n",
    "            \"error_type\": \"Error description\",\n",
    "            \"row_index\": The index of the row where the error occurred,\n",
    "            \"message\": The error message\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    errors = []\n",
    "    storage_client = storage.Client()\n",
    "    blob = storage.Blob.from_string(uri=gcs_jsonl_path, client=storage_client)\n",
    "\n",
    "    with blob.open(\"r\") as f:\n",
    "        for row_index, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                # Check for the presence of the \"messages\" key\n",
    "                if \"messages\" not in data:\n",
    "                    errors.append(\n",
    "                        {\n",
    "                            \"error_type\": \"Missing 'messages' key\",\n",
    "                            \"row_index\": row_index,\n",
    "                            \"message\": f\"Row {row_index} is missing the 'messages' key.\",\n",
    "                        }\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                messages = data[\"messages\"]\n",
    "                # Check if \"messages\" is a list\n",
    "                if not isinstance(messages, list):\n",
    "                    errors.append(\n",
    "                        {\n",
    "                            \"error_type\": \"Invalid 'messages' type\",\n",
    "                            \"row_index\": row_index,\n",
    "                            \"message\": f\"Row {row_index}: 'messages' is not a list.\",\n",
    "                        }\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Validate each message in the \"messages\" list\n",
    "                for message_index, message in enumerate(messages):\n",
    "                    if not isinstance(message, dict):\n",
    "                        errors.append(\n",
    "                            {\n",
    "                                \"error_type\": \"Invalid message format\",\n",
    "                                \"row_index\": row_index,\n",
    "                                \"message\": f\"\"\"Row {row_index},\n",
    "                            message {message_index}: Message is not a dictionary.\"\"\",\n",
    "                            }\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    # Check for required keys in each message dictionary\n",
    "                    if \"role\" not in message or \"content\" not in message:\n",
    "                        errors.append(\n",
    "                            {\n",
    "                                \"error_type\": \"Missing 'role' or 'content' key\",\n",
    "                                \"row_index\": row_index,\n",
    "                                \"message\": f\"Row {row_index}, message {message_index}: \"\n",
    "                                \"Missing 'role' or 'content' key.\",\n",
    "                            }\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    # Check for valid role values\n",
    "                    if message[\"role\"] not in [\"system\", \"user\", \"model\"]:\n",
    "                        errors.append(\n",
    "                            {\n",
    "                                \"error_type\": \"Invalid 'role' value\",\n",
    "                                \"row_index\": row_index,\n",
    "                                \"message\": f\"\"\"Row {row_index}, message {message_index}:\n",
    "                            Invalid 'role' value. Expected 'system', 'user', or 'model'.\"\"\",\n",
    "                            }\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                errors.append(\n",
    "                    {\n",
    "                        \"error_type\": \"JSON Decode Error\",\n",
    "                        \"row_index\": row_index,\n",
    "                        \"message\": f\"Row {row_index}: JSON decoding error: {e}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## 1. Load and Splitting Dataset\n",
    "In this step, we will load the raw data and create training, validation and test sets. Later these datasets will be used to perform  different types of adaptations to Gemini models for the task under consideration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f475c9eceb9"
   },
   "source": [
    "Load the dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k70Yb_XFfcuA"
   },
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"SetFit/bbc-news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "129ed33e0795"
   },
   "source": [
    "Store in Pandas Dataframes the train and test partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAi4NI2MfcuA"
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame(datasets[\"train\"])\n",
    "test = pd.DataFrame(datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "485a5aadee60"
   },
   "source": [
    "We now take a quick look to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcTrjjGTfcuA"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "656325ef857f"
   },
   "source": [
    "We want to check the distribution of the label values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSfs2R-dfcuB"
   },
   "outputs": [],
   "source": [
    "train.label_text.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmAsTaDIfcuB"
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db7764b1824f"
   },
   "source": [
    "We are going to partition the test data into validation and test datasets, in order to have three datasets, namely train, val (validation) and test datasets. To perform evaluations.\n",
    "\n",
    "Test size will be slightly larger than validation, as while fine-tuning Gemini the validation dataset can only be max 256 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRYqeB-kfcuB"
   },
   "outputs": [],
   "source": [
    "val, test = train_test_split(\n",
    "    test, test_size=0.75, shuffle=True, stratify=test[\"label_text\"], random_state=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRtiloYgfcuB"
   },
   "outputs": [],
   "source": [
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3932269eaf0e"
   },
   "source": [
    "Verify that the values of the label column are following a similar distribution, in order to have comparable evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySB5n2ZwfcuB"
   },
   "outputs": [],
   "source": [
    "val.label_text.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec1u71uzfcuB"
   },
   "outputs": [],
   "source": [
    "test.label_text.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2YOsromfcuB"
   },
   "source": [
    "##  2. Evaluation and Experiment Setup\n",
    "We will create the required functions to evaluate our experiments and to log them in Vertex Experiments.\n",
    "\n",
    "\n",
    "### Evaluation Setup\n",
    "For this text classification task, we will use the below classification metrics to evaluate the performance of the models and it different adaptations. We will track the below metrics in our development.\n",
    "\n",
    "- Overall Micro-F1\n",
    "- Overall Macro-F1\n",
    "- Overall Accuracy\n",
    "- Overall Weighted Precision\n",
    "- Overall Weighted Recall\n",
    "- F1-Score (overall and per class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68819c8f056f"
   },
   "source": [
    "The below functions would allow us to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jWbdf4gmfcuB"
   },
   "outputs": [],
   "source": [
    "def predictions_postprocessing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the predicted class label string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The predicted class label string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned class label string.\n",
    "    \"\"\"\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "def evaluate_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    target_column: str = \"label_text\",\n",
    "    predictions_column: str = \"predicted_labels\",\n",
    "    postprocessing: bool = True,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Batch evaluation of predictions, returns a dictionary with the metric.\n",
    "\n",
    "    Args:\n",
    "       - df (pandas.DataFrame):  a pandas dataframe with two mandatory columns, a target column with\n",
    "       the actual true values, and a predictions column with the predicted values.\n",
    "       - target_column (str): column name with the actual ground truth values\n",
    "       - predictions_column (str): column name with the model predictions\n",
    "       - postprocessing (bool): whether to apply postprocessing to predictions.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    if postprocessing:\n",
    "        df[predictions_column] = df[predictions_column].apply(\n",
    "            predictions_postprocessing\n",
    "        )\n",
    "\n",
    "    y_true = df[target_column]\n",
    "    y_pred = df[predictions_column]\n",
    "\n",
    "    metrics_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    overall_macro_f1_score = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    overall_micro_f1_score = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    weighted_precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    weighted_recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": metrics_report[\"accuracy\"],\n",
    "        \"weighted precision\": weighted_precision,\n",
    "        \"weighted recall\": weighted_recall,\n",
    "        \"macro f1\": overall_macro_f1_score,\n",
    "        \"micro f1\": overall_micro_f1_score,\n",
    "    }\n",
    "\n",
    "    categories = [\"business\", \"sport\", \"politics\", \"tech\", \"entertainment\"]\n",
    "    for category in categories:\n",
    "        if category in metrics_report:\n",
    "            metrics[f\"{category}_f1_score\"] = metrics_report[category][\"f1-score\"]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g3sHozyfcuB"
   },
   "source": [
    "### Experiment Setup\n",
    "Before starting the development and experimentation process, we will setup Vertex AI Experiments, in order to log all the experiments we run and compare them using our defined metrics. \n",
    "\n",
    "In this part we will use some of the helper functions we defined in the [helper functions section](#scrollTo=0tKVjsJKfcuA), to create an experiment where we will log all our different runs.\n",
    "\n",
    "For more information about Vertex Experiments, please refer to its [documentation](https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9wT-nkQOeik"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"[your-experiment]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYWhf5YTfcuB"
   },
   "outputs": [],
   "source": [
    "experiment_manager = VertexAIExperimentManager(project=PROJECT_ID, location=LOCATION)\n",
    "experiment_manager.create_experiment(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    experiment_description=\"Fine-tuning Gemini 1.0 Pro for text classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5f7171b3f93"
   },
   "source": [
    "We will create an evaluation DataFrame from our Test dataset, where we will store the predictions from all the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nQymMebwJ-Y"
   },
   "outputs": [],
   "source": [
    "# Create an Evaluation dataframe to store the predictions from all the experiments.\n",
    "df_evals = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfKnRU-SfcuB"
   },
   "source": [
    "## 3. In-Context Adaptation using Gemini models\n",
    "\n",
    "In this section we'll do in-context learning to instruct Gemini models to perform the text classification task under consideration, using zero-shot and few-shot prompt engineering techniques.  \n",
    "\n",
    "The prompts presented in this section are crafted for this task, and in our experiments they demonstrate superior results compared to other simpler prompts.\n",
    "\n",
    "**Before fine-tuning a model, it is important to find the best prompt**: system instructions, examples, structure, etc., for the task under consideration. This will permit to get an understanding of which prompt works the best for the used model, and even boost more the performances when fine-tuning.\n",
    "\n",
    "In this Colab, we are using Gemini 1.0 Pro, in order to compare the performances of the frozen model and after fine-tuning. But you can reuse this code to test also Gemini 1.5 Pro and Gemini 1.5 Flash by changing the model name in the code.\n",
    "\n",
    "**Note:** Prompt Engineering is model-dependent. We recommend you to experiment with different prompting techniques per model. Techniques like Chain-of-Thought can increase performances, as well as Dynamic Few-Shots (using a RAG system to dynamically integrate the examples that are similar to the user input)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts Definition\n",
    "\n",
    "We create the prompts we want to use for our experiments. In this case, we define two: zero-shot and few-shot prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO5XWYDBfcuB"
   },
   "outputs": [],
   "source": [
    "system_prompt_zero_shot = \"\"\"TASK:\n",
    "Classify the text into ONLY one of the following classes [business, entertainment, politics, sport, tech].\n",
    "\n",
    "CLASSES:\n",
    "- business\n",
    "- entertainment\n",
    "- politics\n",
    "- sport\n",
    "- tech\n",
    "\n",
    "INSTRUCTIONS\n",
    "- Respond with ONLY one class.\n",
    "- You MUST use the exact word from the list above.\n",
    "- DO NOT create or use any other classes.\n",
    "- CAREFULLY analyze the text before choosing the best-fitting category from [business, entertainment, politics, sport, tech].\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ne1zh7hfcuB"
   },
   "source": [
    "For the few-shot prompt, we'll randomly pick an example from each category using the `train` dataset we previously computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FApEzz62fcuB"
   },
   "outputs": [],
   "source": [
    "system_prompt_few_shot = f\"\"\"TASK:\n",
    "Classify the text into ONLY one of the following classes [business, entertainment, politics, sport, tech].\n",
    "\n",
    "CLASSES:\n",
    "- business\n",
    "- entertainment\n",
    "- politics\n",
    "- sport\n",
    "- tech\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Respond with ONLY one class.\n",
    "- You MUST use the exact word from the list above.\n",
    "- DO NOT create or use any other classes.\n",
    "- CAREFULLY analyze the text before choosing the best-fitting category from [business, entertainment, politics, sport, tech].\n",
    "\n",
    "EXAMPLES:\n",
    "- EXAMPLE 1:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"business\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"business\", \"label_text\"].iloc[10]}\n",
    "\n",
    "- EXAMPLE 2:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"entertainment\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"entertainment\", \"label_text\"].iloc[10]}\n",
    "\n",
    "- EXAMPLE 3:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"politics\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"politics\", \"label_text\"].iloc[10]}\n",
    "\n",
    "- EXAMPLE 4:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"sport\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"sport\", \"label_text\"].iloc[10]}\n",
    "\n",
    "- EXAMPLE 4:\n",
    "    <user>\n",
    "    {train.loc[train[\"label_text\"] == \"tech\", \"text\"].iloc[10]}\n",
    "    <model>\n",
    "    {train.loc[train[\"label_text\"] == \"tech\", \"label_text\"].iloc[10]}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMVdJ2EffcuC"
   },
   "source": [
    "For the below evaluations, we'll use the respective functions we have already set up. For in-context learning, we recommend to use the validation set to find the optimal performance and then apply it to the test set, to make sure the metrics remain consistent. In this notebook, we'll directly evaluate on the test dataset, as the validation and prompt engineering part has been already done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration Setup\n",
    "\n",
    "We are going to define the generation configuration for doing the text classification task, and keep the same configuration across all of our experiments (both in-context and in-weights). \n",
    "\n",
    "We configure the temperature to 0, to make it as grounded as possible, and max output tokens to 10, as the categories are only one word, we don't need more than that.\n",
    "\n",
    "We are also going to set the safety filters to only block responses which have high severity scores across all four categories. For more information about the Safety configurations, please refer to the [official documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_output_tokens=10, temperature=0)\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF_tas6BfcuC"
   },
   "source": [
    "### 3.1 Gemini 1.0 Pro in-context Evaluation\n",
    "\n",
    "#### Zero-Shot Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2460cc454327"
   },
   "source": [
    "First we will compute the predictions using the frozen model with a prompt without examples (i.e. using the `system_prompt_zero_shot` prompt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLitVLOFqPQe"
   },
   "outputs": [],
   "source": [
    "gem_pro_1_model_zero = GenerativeModel(\n",
    "    \"gemini-1.0-pro-002\",  # e.g. gemini-1.5-pro-001, gemini-1.5-flash-001\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cecce00b47f1"
   },
   "source": [
    "We convert the texts we want to predict to a list and run the online inference parallelizing the calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D2vrHjawsZH"
   },
   "outputs": [],
   "source": [
    "# Get the list of messages to predict\n",
    "messages_to_predict = test[\"text\"].to_list()\n",
    "# Compute the preictions\n",
    "predictions_zero_shot = batch_predict(\n",
    "    messages=messages_to_predict, model=gem_pro_1_model_zero, max_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00a367149153"
   },
   "source": [
    "We store the predictions in the DataFrame we previously defined for storing all the evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufTv6Jx0wwIZ"
   },
   "outputs": [],
   "source": [
    "df_evals[\"gem1.0-zero-shot_predictions\"] = predictions_zero_shot\n",
    "len(predictions_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6964e41ed87"
   },
   "source": [
    "We compute the evaluation metrics for each text, using the zero-shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0h3Tr0LwwBX"
   },
   "outputs": [],
   "source": [
    "# Compute Evaluation Metrics for zero-shot prompt\n",
    "metrics_zero_shot = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"label_text\",\n",
    "    predictions_column=\"gem1.0-zero-shot_predictions\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_zero_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9386224c78b"
   },
   "source": [
    "We finally log the run in the experiment we created in Vertex AI Experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aa6HgjDNwv4y"
   },
   "outputs": [],
   "source": [
    "# Log Experiment with zero-shot Prompt with Gemini 1.0 Pro\n",
    "params = {\n",
    "    \"model\": \"gemini-1.0-pro-002\",\n",
    "    \"adaptation_type\": \"in-context zero-shot\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 10,\n",
    "}\n",
    "\n",
    "experiment_manager.log_run(\n",
    "    run_name=\"gemini-1-0-pro-002-zero-shot\", params=params, metrics=metrics_zero_shot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40b221630b0f"
   },
   "source": [
    "#### Few-shot Evaluation\n",
    "\n",
    "We will now conduct experiments adding examples to our prompt to steer the model. For this, we will use the `system_prompt_few_shot` prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Znl2x6ZSw7cs"
   },
   "outputs": [],
   "source": [
    "# Test Few-Shot, and other prompts/possibilities\n",
    "gem_pro_1_model_few = GenerativeModel(\n",
    "    \"gemini-1.0-pro-002\",\n",
    "    system_instruction=[system_prompt_few_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "692f3a86f3ee"
   },
   "source": [
    "We convert the texts we want to predict to a list and run the online inference parallelizing the calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NM7OU8hZfcuC"
   },
   "outputs": [],
   "source": [
    "predictions_few_shot = batch_predict(\n",
    "    messages=messages_to_predict, model=gem_pro_1_model_few\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d60c0128f81d"
   },
   "source": [
    "We store the predictions on our designated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwUABEdnfcuC"
   },
   "outputs": [],
   "source": [
    "df_evals[\"gem1.0-few-shot_predictions\"] = predictions_few_shot\n",
    "len(predictions_few_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3bc3de3735e"
   },
   "source": [
    "We compute the evaluation metrics for each text, using the zero-shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6CrJ1NzfcuC"
   },
   "outputs": [],
   "source": [
    "# Compute Evaluation Metrics for few-shot prompt\n",
    "metrics_few_shot = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"label_text\",\n",
    "    predictions_column=\"gem1.0-few-shot_predictions\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_few_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9182f894d12"
   },
   "source": [
    "And finally, we also log this run in our experiment, for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qvex8Jd2fcuC"
   },
   "outputs": [],
   "source": [
    "# Log Experiment with Few-Shot Prompt with Gemini 1.0 Pro\n",
    "\n",
    "params = {\n",
    "    \"model\": \"gemini-1.0-pro-002\",\n",
    "    \"adaptation_type\": \"in-context few-shot\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 10,\n",
    "}\n",
    "\n",
    "experiment_manager.log_run(\n",
    "    run_name=\"gemini-1-0-pro-few-shot\", params=params, metrics=metrics_few_shot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs9eHiL5fcuD"
   },
   "source": [
    "## 4. Fine-tuning (Parameter Efficient) Gemini 1.0 Pro\n",
    "Supervised fine-tuning helps adapt foundation models to new tasks using smaller, highly relevant datasets. To ensure success, focus on:\n",
    "\n",
    "- Using domain-specific data: Choose data closely matching your real-world use case.\n",
    "- Accurate labeling: High-quality annotations are crucial.\n",
    "- Clean data: Remove duplicates, fix errors, and ensure relevance to your task.\n",
    "- Diverse but focused examples: Include variety within your target domain, avoiding irrelevant data.\n",
    "- Balanced classes (for classification): Maintain a balance to prevent bias towards a specific class.\n",
    "\n",
    "### 4.1 Prepare tuning and validation datasets for fine-tuning Gemini Models on Vertex AI\n",
    "\n",
    "Training data should be structured within a JSONL file located at a Google Cloud Storage (GCS) URI. Each line (or row) of the JSONL file must adhere to a specific schema: It should contain a \"messages\" array, with objects inside defining a \"role\" (\"system\" for the system context,  \"user\" for user input or \"model\" for model output) and the corresponding text \"content\". For example, a valid data row would look like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You should classify the text into one of the following classes:[business, entertainment]\"\n",
    "      },\n",
    "      { \"role\": \"user\", \"content\": \"Diversify your investment portfolio\" },\n",
    "      { \"role\": \"model\", \"content\": \"business\" }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "The role \"system\" is optional. You can find more information about the dataset format and preparation in the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-about)\n",
    "\n",
    "To run a tuning job, you need to upload your tuning and validation(optional) datasets to a Cloud Storage bucket. You can either create a new Cloud Storage bucket or use an existing one to store dataset files. We recommend that you use a bucket that's in the same Google Cloud project where you plan to tune your model.\n",
    "\n",
    "\n",
    "In this section, we will provide guidelines to prepare the training and validation (optional) datasets based on three options:\n",
    "\n",
    "1. [Option 1] From scratch, using the datasets we loaded and splitted at the beginning of this notebook.\n",
    "\n",
    "1. [Option 2] Providing a function to convert an AutoML Dataset on CSV format to the expected format to fine-tune and validate Gemini Models.\n",
    "\n",
    "1. [Option 3] Providing a function to convert an AutoML Dataset on JSONL format to the expected format to fine-tune and validate Gemini Models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvhGIruMfcuD"
   },
   "source": [
    "#### [Option 1] Prepare tuning and validation datasets from scratch\n",
    "\n",
    "We need to prepare our training and validaiton (optional) datasets for the text classification task. It is recommended to add a system role within the instructions on how to classify. Since we are going to fine-tune the model, the need to add few-shot examples as part of the prompt is eliminated, and therefore we will reuse the `system_prompt_zero_shot` that we used previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8b2056948df"
   },
   "source": [
    "##### Prepare Tuning Dataset for fine-tuning Gemini\n",
    "\n",
    "We will create the tuning dataset by using our previously created `train` DataFrame, and formatting it in the expected structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f72a891e5877"
   },
   "outputs": [],
   "source": [
    "tuning_gemini_df = prepare_tuning_dataset_from_df(\n",
    "    tuning_df=train, system_prompt=system_prompt_zero_shot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d91368a68250"
   },
   "source": [
    "Let's take a look at how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3951e8901a7"
   },
   "outputs": [],
   "source": [
    "tuning_gemini_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89451ccbabec"
   },
   "source": [
    "We store this dataset in Google Cloud Storage to later on pass it when setting up the tuning job.\n",
    "\n",
    "The expected format is JSONL, thus we will convert the pandas DataFrame to JSONL when storing it on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQQM8MZNfcuD"
   },
   "outputs": [],
   "source": [
    "# store tuning dataset in GCS\n",
    "tuning_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/tuning_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "\n",
    "tuning_gemini_df.to_json(tuning_data_gcs_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47671b94765d"
   },
   "source": [
    "In order to make sure it is in the expected format and we won't get later on errors when launching the tuning job, we'll use our custom function validate the dataset has the format and the roles required for tuning Gemini models.\n",
    "\n",
    "If the output is an empty list, it means there were no errors encountered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7b717c38e76"
   },
   "outputs": [],
   "source": [
    "validate_gemini_tuning_jsonl(gcs_jsonl_path=tuning_data_gcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab4c1c9e5b7a"
   },
   "source": [
    "##### Prepare Validation Dataset for Fine-tuning Gemini 1.0 Pro\n",
    "We do the same but now using the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b48ac0b8d5b0"
   },
   "outputs": [],
   "source": [
    "validation_gemini_df = prepare_tuning_dataset_from_df(\n",
    "    tuning_df=val, system_prompt=system_prompt_zero_shot\n",
    ")\n",
    "validation_gemini_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1553424ac2e"
   },
   "outputs": [],
   "source": [
    "validation_gemini_df.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3323aff3540c"
   },
   "source": [
    "We store this dataset in Google Cloud Storage to later on pass it when setting up the tuning job.\n",
    "\n",
    "The expected format is JSONL, thus we will convert the pandas DataFrame to JSONL when storing it on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGnLQB24fcuD"
   },
   "outputs": [],
   "source": [
    "# store validation dataset in GCS\n",
    "validation_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/validation_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "validation_gemini_df.to_json(validation_data_gcs_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77e8f4c89828"
   },
   "source": [
    "In order to make sure it is in the expected format and we won't get later on errors when launching the tuning job, we'll use our custom function validate the dataset has the format and the roles required for tuning Gemini models.\n",
    "\n",
    "If the output is an empty list, it means there were no errors encountered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5191953bb448"
   },
   "outputs": [],
   "source": [
    "validate_gemini_tuning_jsonl(gcs_jsonl_path=validation_data_gcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebee4ef624a0"
   },
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D4sezIqfcuD"
   },
   "source": [
    "#### [Option 2] Transform AutoML CSV training dataset format to the expected data format for fine-tuning Gemini.\n",
    "If you were previously using Vertex AI AutoML for text classification, and you have your data in the below csv format expected by AutoML:\n",
    "\n",
    "```\n",
    "[ml_use],gcs_file_uri|\"inline_text\",label\n",
    "```\n",
    "\n",
    "```\n",
    "test,\"inline_text\",label1\n",
    "test,\"inline_text\",label2\n",
    "training,\"inline_text\",label3\n",
    "validation,\"inline_text\",label1\n",
    "```\n",
    "\n",
    " In the file `data_transformations_tuning.py` we have the function ` def convert_tuning_dataset_from_automl_csv(...)` to convert AutoML CSV datasets for text classification to the format expected for the tuning dataset to fine-tune Gemini models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19f64486390c"
   },
   "source": [
    "##### Prepare tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5mzsiDl0CMq"
   },
   "outputs": [],
   "source": [
    "# Usage Example for Training dataset\n",
    "gcs_path_automl_dataset = (\n",
    "    \"gs://<your-bucket-path>/<your-data>.csv\"  # @param {type: \"string\"}\n",
    ")\n",
    "df_gemini_tuning = convert_tuning_dataset_from_automl_csv(\n",
    "    automl_gcs_csv_path=gcs_path_automl_dataset,\n",
    "    system_prompt=system_prompt_zero_shot,\n",
    "    partition=\"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea54c13d61a8"
   },
   "outputs": [],
   "source": [
    "df_gemini_tuning.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4cd727c6fb4"
   },
   "source": [
    "We store this dataset in Google Cloud Storage to later on pass it when setting up the tuning job.\n",
    "\n",
    "The expected format is JSONL, thus we will convert the pandas DataFrame to JSONL when storing it on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8f2ed3f7ecf"
   },
   "outputs": [],
   "source": [
    "# store tuning dataset in GCS\n",
    "gemini_tuning_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/tuning_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "df_gemini_tuning.to_json(gemini_tuning_data_gcs_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a64207f7529"
   },
   "source": [
    "In order to make sure it is in the expected format and we won't get later on errors when launching the tuning job, we'll use our custom function validate the dataset has the format and the roles required for tuning Gemini models.\n",
    "\n",
    "If the output is an empty list, it means there were no errors encountered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05eff9b207e1"
   },
   "outputs": [],
   "source": [
    "validate_gemini_tuning_jsonl(gcs_jsonl_path=gemini_tuning_data_gcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d93823802fa"
   },
   "source": [
    "##### Prepare Validation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c21d1d425c20"
   },
   "source": [
    "We will repeat the same process for the validation dataset in case there is one available. It is not mandatory to provide a validation dataset when fine-tuning Gemini, but rather optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIBlBE2q0yYp"
   },
   "outputs": [],
   "source": [
    "# Usage Example for validation dataset\n",
    "gcs_path_automl_dataset = (\n",
    "    \"gs://<your-bucket-path>/<your-data>.csv\"  # @param {type: \"string\"}\n",
    ")\n",
    "df_gemini_validation = convert_tuning_dataset_from_automl_csv(\n",
    "    automl_gcs_csv_path=gcs_path_automl_dataset,\n",
    "    system_prompt=system_prompt_zero_shot,\n",
    "    partition=\"validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3384b71bef8a"
   },
   "source": [
    "We store this dataset in Google Cloud Storage to later on pass it when setting up the tuning job.\n",
    "\n",
    "The expected format is JSONL, thus we will convert the pandas DataFrame to JSONL when storing it on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0abb34eb080b"
   },
   "outputs": [],
   "source": [
    "# store tuning dataset in GCS\n",
    "gemini_validation_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/validation_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "df_gemini_validation.to_json(\n",
    "    gemini_validation_data_gcs_path, orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d9afbf270ba"
   },
   "source": [
    "In order to make sure it is in the expected format and we won't get later on errors when launching the tuning job, we'll use our custom function validate the dataset has the format and the roles required for tuning Gemini models.\n",
    "\n",
    "If the output is an empty list, it means there were no errors encountered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9492a4c71837"
   },
   "outputs": [],
   "source": [
    "validate_gemini_tuning_jsonl(gcs_jsonl_path=gemini_validation_data_gcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "191fbdd4bc30"
   },
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpuZz3jCfcuD"
   },
   "source": [
    "####  [Option 3] AutoML JSONL training dataset format to Gemini tuning data format\n",
    "\n",
    "If you were previously using Vertex AI AutoML for text classification, and you have your data in the below JSONL format expected by AutoML:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"classificationAnnotation\": {\n",
    "    \"displayName\": \"label\"\n",
    "  },\n",
    "  \"textContent\": \"inline_text\",\n",
    "  \"dataItemResourceLabels\": {\n",
    "    \"aiplatform.googleapis.com/ml_use\": \"training|test|validation\"\n",
    "  }\n",
    "}\n",
    "{\n",
    "  \"classificationAnnotation\": {\n",
    "    \"displayName\": \"label2\"\n",
    "  },\n",
    "  \"textContent\": \"inline_text\",\n",
    "  \"dataItemResourceLabels\": {\n",
    "    \"aiplatform.googleapis.com/ml_use\": \"training|test|validation\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    " In the file `data_transformations_tuning.py` we have the function ` def convert_tuning_dataset_from_automl_jsonl(...)` to convert  AutoML JSONL datasets for text classification to the format expected for the tuning dataset to fine-tune Gemini models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05b04e4270d8"
   },
   "source": [
    "##### Prepare Tuning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3081dbe973e6"
   },
   "outputs": [],
   "source": [
    "# Usage Example for Training dataset\n",
    "\n",
    "gcs_path_automl_dataset = (\n",
    "    \"gs://<your-bucket-path>/<your-data>.jsonl\"  # @param {type: \"string\"}\n",
    ")\n",
    "\n",
    "df_gemini_tuning = convert_tuning_dataset_from_automl_jsonl(\n",
    "    project_id=PROJECT_ID,\n",
    "    automl_gcs_jsonl_path=gcs_path_automl_dataset,\n",
    "    system_prompt=system_prompt_zero_shot,\n",
    "    partition=\"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "368a0a0d26ea"
   },
   "outputs": [],
   "source": [
    "df_gemini_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c01550efe4b"
   },
   "source": [
    "We store this dataset in Google Cloud Storage to later on pass it when setting up the tuning job.\n",
    "\n",
    "The expected format is JSONL, thus we will convert the pandas DataFrame to JSONL when storing it on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0QMNGCvBR4f"
   },
   "outputs": [],
   "source": [
    "# store tuning dataset in GCS\n",
    "gemini_tuning_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/tuning_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "df_gemini_tuning.to_json(gemini_tuning_data_gcs_path, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e339802cd01"
   },
   "source": [
    "In order to make sure it is in the expected format and we won't get later on errors when launching the tuning job, we'll use our custom function validate the dataset has the format and the roles required for tuning Gemini models.\n",
    "\n",
    "If the output is an empty list, it means there were no errors encountered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae1985e9a251"
   },
   "outputs": [],
   "source": [
    "validate_gemini_tuning_jsonl(gcs_jsonl_path=gemini_tuning_data_gcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "616487009b94"
   },
   "source": [
    "##### Prepare Validation Dataset\n",
    "Now we repeat the same process but with the validation dataset. When fine-tuning Gemini you can pass on two datasets: Training/Tuning Dataset (mandatory) and Validation Dataset (optional). If the validation dataset is provided, you can monitor also the metrics on this dataset during  the tuning process, however providing a validaiton dataset is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e23edfa617ee"
   },
   "outputs": [],
   "source": [
    "gcs_path_automl_dataset = (\n",
    "    \"gs://<your-bucket-path>/<your-data>.jsonl\"  # @param {type: \"string\"}\n",
    ")\n",
    "\n",
    "df_gemini_validation = convert_tuning_dataset_from_automl_jsonl(\n",
    "    project_id=PROJECT_ID,\n",
    "    automl_gcs_jsonl_path=gcs_path_automl_dataset,\n",
    "    system_prompt=system_prompt_zero_shot,\n",
    "    partition=\"validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1e4aa7699a2"
   },
   "source": [
    "We store this dataset in Google Cloud Storage to later on pass it when setting up the tuning job.\n",
    "\n",
    "The expected format is JSONL, thus we will convert the pandas DataFrame to JSONL when storing it on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWPTSFceBSJH"
   },
   "outputs": [],
   "source": [
    "# store tuning dataset in GCS\n",
    "gemini_validation_data_gcs_path = f\"gs://{BUCKET_NAME}/tuning_experiments/validation_dataset_gemini.jsonl\"  # @param {type: \"string\"}\n",
    "df_gemini_validation.to_json(\n",
    "    gemini_validation_data_gcs_path, orient=\"records\", lines=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41b110afaf03"
   },
   "source": [
    "In order to make sure it is in the expected format and we won't get later on errors when launching the tuning job, we'll use our custom function validate the dataset has the format and the roles required for tuning Gemini models.\n",
    "\n",
    "If the output is an empty list, it means there were no errors encountered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c94d573bba2"
   },
   "outputs": [],
   "source": [
    "validate_gemini_tuning_jsonl(gcs_jsonl_path=gemini_validation_data_gcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eff7b6438ae2"
   },
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51vqjijWfcuD"
   },
   "source": [
    "### 4.2 Start fine-tuning job\n",
    "\n",
    "- source_model: Specifies the base Gemini model version you want to fine-tune.\n",
    "- train_dataset: Path to your training data in JSONL format.\n",
    "\n",
    "Optional parameters\n",
    "\n",
    "- validation_dataset: If provided, this data is used to evaluate the model during tuning.\n",
    "- adapter_size: A higher adapter size means more trainable parameters.\n",
    "- epochs: The number of training epochs to run.\n",
    "- learning_rate_multiplier: A value to scale the learning rate during training.\n",
    "\n",
    "We recommend to make a different set of experiments with different hyperparameter.  The below configurations are recommended to experiment based on our experiments, if your dataset is in the size of 1000s and you are including the system role in your dataset.\n",
    "\n",
    "1. epochs: 4, learning_rate_multiplier: 1, adapter_size: 1\n",
    "1. epochs: 12, learning_rate_multiplier: 4,  adapter_size: 1\n",
    "\n",
    "If you are not including system role in your dataset, and only role user with the raw text and role models with the label, then we recommend to increase the adapter size. The below are some configurations you can start experimenting with.\n",
    "\n",
    "1. epochs: 12, learning_rate_multiplier: 4, adapter_size: 4\n",
    "1. epochs: 24, learning_rate_multiplier: 4,  adapter_size: 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe0d3b83aeed"
   },
   "source": [
    "First, we set the parameters values for the first fine tuning job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcb7bd25c14d"
   },
   "outputs": [],
   "source": [
    "# Tune a model using `train` method.\n",
    "\n",
    "tuned_model_name = \"<add-name-for-tuned-model>\"  # @param {type: \"string\"}\n",
    "epochs = 4  # @param\n",
    "learning_rate_multiplier = 1  # @param\n",
    "adapter_size = 1  # @param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d82dfffa4d67"
   },
   "source": [
    "Now, we trigger the tuning job. After running the below cell, you'll get a link to the console where you can monitor the tuning job such as metrics, and get statistics of the dataset used for tuning. After the tuning job finishes, you can also find the details for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhfUjyF2fcuD"
   },
   "outputs": [],
   "source": [
    "sft_tuning_job = sft.train(\n",
    "    tuned_model_display_name=tuned_model_name,\n",
    "    source_model=\"gemini-1.0-pro-002\",\n",
    "    train_dataset=tuning_data_gcs_path,\n",
    "    # Optional:\n",
    "    validation_dataset=validation_data_gcs_path,\n",
    "    epochs=epochs,\n",
    "    learning_rate_multiplier=learning_rate_multiplier,\n",
    "    adapter_size=adapter_size,\n",
    ")\n",
    "\n",
    "# Get the tuning job info.\n",
    "sft_tuning_job.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIgb_lalfcuD"
   },
   "outputs": [],
   "source": [
    "# Get the resource name of the tuning job\n",
    "sft_tuning_job_name = sft_tuning_job.resource_name\n",
    "sft_tuning_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xxo0CWRfcuD"
   },
   "source": [
    "### 4.3 Get the tuned model and test it\n",
    "\n",
    "To retrieve the full path from the console. You can go to [Vertex AI Studio tuning section](https://console.cloud.google.com/vertex-ai/generative/language/tuning?_ga=2.250955014.1608754049.1722498783-327343626.1722249232) and select the region where you launched your job, click on your tuning job and go to details. The last part of the Tuning Job path is the tuning job ID. Alternatively, you can also select the entire path and replace it directly as an argument for `sft.SupervisedTuningJob(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bHSuQEZfcuD"
   },
   "outputs": [],
   "source": [
    "# Get tuning job\n",
    "TUNING_JOB_ID = \"<add your tuning job id>\"  # @param example 952462564720115710\n",
    "sft_tuning_job = sft.SupervisedTuningJob(\n",
    "    f\"projects/{PROJECT_ID}/locations/{LOCATION}/tuningJobs/{TUNING_JOB_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahcGD1KxfcuD"
   },
   "outputs": [],
   "source": [
    "# tuned model endpoint name\n",
    "tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\n",
    "tuned_model_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LchxG2QufcuD"
   },
   "outputs": [],
   "source": [
    "# tuned model name\n",
    "tuned_model_name = sft_tuning_job.tuned_model_name\n",
    "tuned_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfc143bd2866"
   },
   "source": [
    "Initiate the tuned model and test it on a single example. We will use the same generation and safety configuration as when doing in-context learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e00bee65f433"
   },
   "outputs": [],
   "source": [
    "tuned_gemini_pro = GenerativeModel(\n",
    "    tuned_model_endpoint_name,\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOzD7dG2fcuE"
   },
   "outputs": [],
   "source": [
    "response = tuned_gemini_pro.generate_content([test[\"text\"].iloc[4]], stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZw7iucdfcuE"
   },
   "outputs": [],
   "source": [
    "print(\"predicted\", response.text)\n",
    "print(\"ground truth\", test[\"label_text\"].iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbOY6eEgfcuE"
   },
   "source": [
    "### 4.4 Run evaluations on tuned model and log experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8tVNr4SfcuE"
   },
   "outputs": [],
   "source": [
    "# Get the list of messages to predict\n",
    "messages_to_predict = test[\"text\"].to_list()\n",
    "# Compute the predictions using the zero-shot prompt\n",
    "predictions_tuned_model = batch_predict(\n",
    "    messages=messages_to_predict, model=tuned_gemini_pro, max_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX-t7KXxfcuE"
   },
   "outputs": [],
   "source": [
    "df_evals[\"tuned-gem1.0-ep4-lrm1-rank4\"] = predictions_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ri9X1ZOfcuE"
   },
   "outputs": [],
   "source": [
    "metrics_tuned_gemini = evaluate_predictions(\n",
    "    df_evals.copy(),\n",
    "    target_column=\"label_text\",\n",
    "    predictions_column=\"tuned-gem1.0-ep4-lrm1-rank4\",\n",
    "    postprocessing=True,\n",
    ")\n",
    "metrics_tuned_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0DWM0nmfcuE"
   },
   "outputs": [],
   "source": [
    "# Log Experiment with zero-shot Prompt with Gemini 1.5 Pro\n",
    "\n",
    "params = {\n",
    "    \"model\": tuned_model_name,\n",
    "    \"adaptation_type\": \"fine-tuning Gemini 1.0 Pro 002\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 10,\n",
    "    \"epochs\": epochs,\n",
    "    \"lrm\": learning_rate_multiplier,\n",
    "    \"adapter_size\": adapter_size,\n",
    "}\n",
    "\n",
    "experiment_manager.log_run(\n",
    "    run_name=\"<your-experiment-run-name\",  # add your experiment name\n",
    "    params=params,\n",
    "    metrics=metrics_tuned_gemini,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cIFYQ-3fcuE"
   },
   "source": [
    "## 5.  Evaluation comparisons\n",
    "\n",
    "To assess the performance of your experiments in Vertex AI, you have two primary options. You can programmatically retrieve a comprehensive DataFrame containing all experiments and their associated metrics for in-depth analysis. Alternatively, Vertex AI offers a user-friendly visual UI enabling you to compare experiments, select specific runs for side-by-side comparisons, and gain rapid insights. For detailed instructions on both approaches, refer to the [Vertex AI documentation on evaluation comparisons](https://cloud.google.com/vertex-ai/docs/experiments/compare-analyze-runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vF948vqJfcuE"
   },
   "outputs": [],
   "source": [
    "df_experiments = experiment_manager.get_experiments_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dtTMBKnfcuE"
   },
   "outputs": [],
   "source": [
    "df_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUNM5LJA1OZq"
   },
   "source": [
    "> Note: In the experiments with this dataset the most performant model was achieved by fine-tuning Gemini 1.0 Pro  with the below parameters:\n",
    "\n",
    "```\n",
    "epochs=6, learning_rate_multiplier= 1, and adapter_size=4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZzNV7fnuJsy"
   },
   "source": [
    "## [Optional] 6. Heuristics for Computing Confidence Scores\n",
    "\n",
    "Due to the multitask essence of LLMs computing confidence scores is not as straightforward as it is with traditional predictive AI. Gemini models do not expose logprobs for the time being. However, the below snippets provide some options to use as a proxy for confidence scores in your predictions. You can expand these options to your own use cases and needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW7wPWQWuQT4"
   },
   "source": [
    "### [Option 1] -  Getting multiple responses from the model and generate a majority voting ratio\n",
    "\n",
    "The overall idea is to generate different answers with the same model. Then pick the most \"voted/returned\" answer, and calculate its \"confidence score\" by dividing the number of votes among the total number of responses/candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acbe5d5c8a48"
   },
   "source": [
    "First we will define the function that will help us do the prediction and the numerical confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rViPbiseuOmr"
   },
   "outputs": [],
   "source": [
    "def get_prediction_with_numeric_score(\n",
    "    text_to_predict: str, model: Any, candidate_counts: int\n",
    ") -> Dict[str, Union[float, str]]:\n",
    "    \"\"\"\n",
    "    Generates multiple predictions from a model and determines\n",
    "    the most frequent response along with its confidence score.\n",
    "\n",
    "    Args:\n",
    "        text_to_predict: The input text for which to generate predictions.\n",
    "        model: The prediction model to use.\n",
    "        candidate_counts: The number of predictions to generate.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the majority prediction and its confidence score.\n",
    "        For example: {\"prediction\": \"business\", \"confidence_score\": 0.75}\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    for _ in range(candidate_counts):\n",
    "        responses.append(model.generate_content(text_to_predict).text)\n",
    "\n",
    "    counts = Counter(responses)\n",
    "    max_value = max(counts.values())\n",
    "    majority_response = [key for key in counts if counts[key] == max_value][0]\n",
    "    confidence = max_value / len(responses)\n",
    "    result = {\"prediction\": majority_response, \"confidence_score\": confidence}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2960c03f3be7"
   },
   "source": [
    "Initialize the model to predict the class. In this example, we will use the tuned Gemini model we created before. We will use the same configurations used when doing in-context and in-weights learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ae1e8c36764"
   },
   "outputs": [],
   "source": [
    "tuned_gemini_pro = GenerativeModel(\n",
    "    tuned_model_endpoint_name,\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a1d74a2a393"
   },
   "source": [
    "Get the predictions with its corresponding confidence score for an example text in our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiN6cX3VuXeE"
   },
   "outputs": [],
   "source": [
    "res = get_prediction_with_numeric_score(\n",
    "    text_to_predict=test[\"text\"].iloc[473], model=tuned_gemini_pro, candidate_counts=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8796e7d50560"
   },
   "source": [
    "Print the response and the ground truth for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSVIIVHquYOL"
   },
   "outputs": [],
   "source": [
    "print(\"Predicted Response with Confidence Score: \\n\", res)\n",
    "print(\"Ground Truth:\\n\", test[\"label_text\"].iloc[473])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEWe0M-juZyx"
   },
   "source": [
    "### [Option 2] -  Generating \"Verbal Confidences\" with an LLM\n",
    "\n",
    "The idea is to make 2-calls per prediction, one for predicting the class, and a second one to ask the LLM to judge how confident it is about it, giving as options verbal confidences like \"low\", \"medium\" and \"high\".\n",
    "\n",
    "In this example, we will use our tuned Gemini model to predict the class and frozen Gemini 1.5 Pro to judge the prediction verbally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4593a06d3708"
   },
   "source": [
    "First we will define the function that will help us do the prediction and the verbal confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGWzgnx1udB0"
   },
   "outputs": [],
   "source": [
    "def get_prediction_with_verbal_score(\n",
    "    text_to_predict: str,\n",
    "    model_to_predict_class: Any,\n",
    "    model_to_eval_prediction: Any,\n",
    "    possible_classes: List[str] = [\n",
    "        \"business\",\n",
    "        \"entertainment\",\n",
    "        \"sport\",\n",
    "        \"tech\",\n",
    "        \"politics\",\n",
    "    ],\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generates a prediction and then evaluates its confidence using a separate model.\n",
    "\n",
    "    Args:\n",
    "        text_to_predict: The input text for which to generate predictions.\n",
    "        model_to_predict_class: The model to predict the class.\n",
    "        model_to_eval_prediction: The model to evaluate the confidence of the prediction.\n",
    "        possible_classes: A list of possible classes.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the prediction and its verbal confidence score.\n",
    "        For example: {\"prediction\": \"business\", \"verbal_score\": \"very confident\"}\n",
    "    \"\"\"\n",
    "    prediction = model_to_predict_class.generate_content(text_to_predict).text\n",
    "    remaining_classes = possible_classes.copy()\n",
    "    remaining_classes.remove(prediction)\n",
    "    formatted_prompt = f\"\"\"\n",
    "    TEXT:\n",
    "    {text_to_predict}\n",
    "\n",
    "    PREDICTED CLASS:\n",
    "    {prediction}\n",
    "\n",
    "    OTHER POSSIBLE CLASSES:\n",
    "    {remaining_classes}\n",
    "    \"\"\"\n",
    "    confidence = model_to_eval_prediction.generate_content(formatted_prompt).text\n",
    "    result = {\"prediction\": prediction, \"verbal_score\": confidence}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d31f34979788"
   },
   "source": [
    "Configure the model parameters and initialize the model to predict the class. In this example, we will use the tuned Gemini model we created before. We will use the same configurations used when doing in-context and in-weights learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8ginB-BufDT"
   },
   "outputs": [],
   "source": [
    "model_to_predict_class = GenerativeModel(\n",
    "    tuned_model_endpoint_name,\n",
    "    system_instruction=[system_prompt_zero_shot],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "778511fcc385"
   },
   "source": [
    "Define the Prompt to steer the evaluation and verbal confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "USxu9I0nugwU"
   },
   "outputs": [],
   "source": [
    "# Define the configurations for the model which will evaluate the predictions\n",
    "\n",
    "eval_prompt = \"\"\"\n",
    "You will get a text about a particular topic, the predicted class for the topic and a list of the other different classes that the model could have chosen.\n",
    "Your task is to judge how well the predicted class fitted the text, based on the other possible classes.\n",
    "You need to evaluate and judge your prediction, indicating how confident you are with your answer. You will judge the prediction as follows:\n",
    "\n",
    "- If you are confident the text is correctly labeled with the given prediction, then respond with \"High\"\n",
    "- If it can be that the model could match other classes, or you are not very sure the class corresponds to the text, then respond with \"Medium\"\n",
    "- If you believe it makes no sense the class predicted for that text, then respond with \"Low\".\n",
    "\n",
    "You MUST only output \"High\", \"Medium\" or \"Low\" without any further explanation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b1573955467"
   },
   "source": [
    "Initialize the model to be used for computing the verbal confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_eval_class = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\",\n",
    "    system_instruction=[eval_prompt],\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function to compute the prediction including its verbal score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf8liMlCumTl"
   },
   "outputs": [],
   "source": [
    "# Call the function to get the predictions with verbal score\n",
    "res_verbal_conf = get_prediction_with_verbal_score(\n",
    "    text_to_predict=test[\"text\"].iloc[473],\n",
    "    model_to_predict_class=model_to_predict_class,\n",
    "    model_to_eval_prediction=model_to_eval_class,\n",
    "    possible_classes=[\"business\", \"entertainment\", \"sport\", \"tech\", \"politics\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUHITtnXukCx"
   },
   "outputs": [],
   "source": [
    "print(\"Predicted Response with Verbal Score: \\n\", res_verbal_conf)\n",
    "print(\"Ground Truth:\\n\", test[\"label_text\"].iloc[473])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPX8n4Wp0_uN"
   },
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-SUNrp_1MJ3"
   },
   "source": [
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "Refer to this [instructions](https://cloud.google.com/vertex-ai/docs/tutorials/image-classification-custom/cleanup#delete_resources) to delete the resources from console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzpTkkai1LG7"
   },
   "outputs": [],
   "source": [
    "# Delete Experiment.\n",
    "delete_experiments = True\n",
    "if delete_experiments:\n",
    "    experiments_list = aiplatform.Experiment.list()\n",
    "    for experiment in experiments_list:\n",
    "        if experiment.resource_name == EXPERIMENT_NAME:\n",
    "            print(experiment.resource_name)\n",
    "            experiment.delete()\n",
    "            break\n",
    "\n",
    "print(\"***\" * 10)\n",
    "\n",
    "# Delete Endpoint.\n",
    "delete_endpoint = True\n",
    "# If force is set to True, all deployed models on this\n",
    "# Endpoint will be first undeployed.\n",
    "if delete_endpoint:\n",
    "    for endpoint in aiplatform.Endpoint.list():\n",
    "        if endpoint.resource_name == tuned_model_endpoint_name:\n",
    "            print(endpoint.resource_name)\n",
    "            endpoint.delete(force=True)\n",
    "            break\n",
    "\n",
    "print(\"***\" * 10)\n",
    "\n",
    "# Delete Model.\n",
    "delete_model = True\n",
    "if delete_model:\n",
    "    # Remove version from model name.\n",
    "    tuned_model_name = tuned_model_name.split(\"@\")[0]\n",
    "    for model in aiplatform.Model.list():\n",
    "        if model.resource_name == tuned_model_name:\n",
    "            print(model.resource_name)\n",
    "            model.delete()\n",
    "            break\n",
    "\n",
    "print(\"***\" * 10)\n",
    "\n",
    "# Delete Cloud Storage Bucket.\n",
    "delete_bucket = True\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "gemini_supervised_finetuning_text_classification.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
